{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "OPEN_AI_KEY = os.environ.get('OPEN_AI_KEY')\n",
    "client = OpenAI(api_key=OPEN_AI_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a system that can generate chat prompts that understands values in Chinese communication. This includes indirect communication, which relies on subtlety and hints and context, as well as showing respect in front of elders, as well as humility. \"},\n",
    "    {\"role\": \"system\", \"content\": \"Here are 3 examples: Input:\\nCONTEXT: You are a caregiver in a nursing home, conversing with an elderly Chinese resident, Mrs. Li, in English. You notice she hasn't finished her food from yesterday.\\n Mrs. Li: Hello!\\n Response:\\n Hello Mrs. Li. Was the food from yesterday to your liking?\\n \\n Input:\\n CONTEXT: You are a hotel receptionist, speaking to an elderly Chinese couple that is checking out. They have not paid yet, but are insisting they have. \\n Woman: We would like to check out, we paid yesterday.\\n Response:\\n I apologize if there's been a misunderstanding. I will double check our system again.\\n \\n Input:\\n CONTEXT: You are conversing with a Chinese coworker to provide feedback on their work, which is a little lacking. \\n Coworker: Hello!\\n Response:\\n Hello, how are you? I've reviewed your work. It's quite comprehensive. Would you like to go over this section though?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me 40 chat prompts of unique scenarios that exhibit values in Chinese communication, especially the values of indirectness and respect to elders. Use the format of the examples given with the context.\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. CONTEXT: You are at a family gathering and your elderly grandmother has cooked a meal for everyone.\\nGrandmother: Please, have a taste of my dishes.\\nResponse: Grandmother, your cooking is always a delight. I'll be sure to savor every bite.\\n\\n2. CONTEXT: You are at a meeting with your Chinese boss, who is suggesting changes to your project that you strongly disagree with.\\nBoss: I suggest we take a different approach to this project.\\nResponse: I appreciate your insights, and I will consider different perspectives moving forward.\\n\\n3. CONTEXT: You are chatting with a Chinese friend who is feeling unwell but doesn't want to admit it directly.\\nFriend: I'm just feeling a bit tired today.\\nResponse: I understand, please take care of yourself and get some rest if you need to.\\n\\n4. CONTEXT: You are discussing a potential business partnership with an elderly Chinese investor who seems hesitant.\\nInvestor: I have some concerns about this proposal.\\nResponse: I value your insights and take your concerns seriously. Let's address them together.\\n\\n5. CONTEXT: You are giving feedback to a younger Chinese colleague on their presentation, which lacked clarity.\\nColleague: How do you think my presentation went?\\nResponse: Your presentation had some good points, perhaps we can work on enhancing the structure for better impact.\\n\\n6. CONTEXT: You are speaking to your elderly Chinese neighbor who is struggling with their garden.\\nNeighbor: I seem to be having some trouble keeping my garden in order.\\nResponse: It's admirable that you manage your garden so diligently. Would you like some help with it?\\n\\n7. CONTEXT: You are discussing a potential collaboration with a Chinese business partner who seems hesitant about the terms.\\nPartner: I am unsure if these terms align with our goals.\\nResponse: I respect your perspective and am open to discussing how we can find a mutually beneficial agreement.\\n\\n8. CONTEXT: You are catching up with an older Chinese relative who seems reluctant to share details about their health.\\nRelative: I'm managing, just feeling a bit off recently.\\nResponse: Your well-being is important to us. If there's anything you need, please don't hesitate to let us know.\\n\\n9. CONTEXT: You are in a meeting with a Chinese team discussing a new project timeline that some members are concerned about.\\nTeam member: I'm not sure if the proposed timeline is realistic.\\nResponse: I appreciate your input. Let's explore how we can adjust the timeline to ensure successful completion.\\n\\n10. CONTEXT: You are receiving feedback from your Chinese professor on a research paper that needs improvement.\\nProfessor: Your paper shows potential but requires more depth.\\nResponse: Thank you for your guidance. I will work on expanding the analysis to meet the required standards.\\n\\n11. CONTEXT: You are discussing travel plans with your elderly Chinese parents, who are hesitant about a destination you suggested.\\nParents: We are a bit concerned about traveling there.\\nResponse: I understand your concerns. Let's explore other options that suit everyone's preferences.\\n\\n12. CONTEXT: You are negotiating a contract with a Chinese client who is hesitant about the proposed terms.\\nClient: I'm not entirely comfortable with these terms.\\nResponse: Your feedback is valuable. Let's find a solution that meets both our needs and honors our partnership.\\n\\n13. CONTEXT: You are at a family dinner where your elderly Chinese grandparents are receiving compliments on their cooking.\\nGrandparents: It's nothing special, just simple dishes.\\nResponse: Your culinary skills never fail to impress us. We are grateful for this delicious meal.\\n\\n14. CONTEXT: You are discussing a project with your Chinese supervisor, who suggests changes to your team's approach.\\nSupervisor: I recommend we consider a different strategy for this project.\\nResponse: Your insights are valuable, and I will discuss the proposed changes with the team for further consideration.\\n\\n15. CONTEXT: You are chatting with a Chinese friend who is going through a difficult time but is hesitant to share details.\\nFriend: I'm just dealing with some personal matters at the moment.\\nResponse: Your well-being is important to me. Whenever you're ready to talk, I'm here to listen.\\n\\n16. CONTEXT: You are presenting a new idea to a Chinese business partner who seems uncertain about its feasibility.\\nPartner: I have reservations about the practicality of this idea.\\nResponse: I appreciate your feedback. Let's collaborate on refining the idea to address any concerns.\\n\\n17. CONTEXT: You are discussing career goals with your Chinese mentor, who offers suggestions for your professional development.\\nMentor: Have you considered exploring opportunities in a different industry?\\nResponse: Your mentorship is invaluable to me. I will reflect on your suggestion and explore new possibilities.\\n\\n18. CONTEXT: You are seeking advice from your elderly Chinese relative on a personal matter.\\nRelative: I'm not sure if I have all the answers, but I'll do my best to help.\\nResponse: Your wisdom and guidance mean a lot to me. I appreciate your perspective on this.\\n\\n19. CONTEXT: You are in a meeting with a Chinese client who expresses concerns about the project's progress.\\nClient: I'm worried about the timeline and deliverables.\\nResponse: Your feedback is important. Let's discuss how we can address these concerns and ensure a successful outcome.\\n\\n20. CONTEXT: You are giving feedback to a Chinese colleague on a project that needs improvement.\\nColleague: How do you think we can enhance the impact of our work?\\nResponse: Your dedication to the project is commendable. Let's brainstorm together on ways to elevate our results.\\n\\n21. CONTEXT: You are at a family gathering, and your Chinese grandmother insists on serving you more food despite your protests.\\nGrandmother: You must have more, you're too thin!\\nResponse: Your care and generosity are always appreciated, Grandma. I'll gladly indulge in one more helping.\\n\\n22. CONTEXT: You are discussing a partnership proposal with a Chinese business associate who seeks more clarity on the terms.\\nAssociate: I need a better understanding of the proposed agreement.\\nResponse: Your thoroughness is commendable. Let's review the terms together to ensure mutual understanding and alignment.\\n\\n23. CONTEXT: You are sharing updates with your elderly Chinese neighbor, who is concerned about recent neighborhood changes.\\nNeighbor: I'm not sure how I feel about the new developments nearby.\\nResponse: Your perspective is important. Let's discuss how we can address any concerns and maintain the sense of community we cherish.\\n\\n24. CONTEXT: You are providing feedback to a Chinese team member on a collaborative project that requires additional effort.\\nTeam member: How can we improve our collaboration for better results?\\nResponse: Your dedication to the project is evident. Let's work together on enhancing our teamwork to achieve our goals efficiently.\\n\\n25. CONTEXT: You are discussing potential investments with a Chinese partner who expresses reservations about the market volatility.\\nPartner: I'm concerned about the risks involved in this investment.\\nResponse: Your caution is understandable. Let's reevaluate the investment strategy together to mitigate risks and maximize potential returns.\\n\\n26. CONTEXT: You are receiving feedback from your Chinese professor on a group project that needs refinement.\\nProfessor: Your group's project shows promise but lacks cohesiveness.\\nResponse: Thank you for your guidance. We will collaborate on improving the project's structure and content to meet the required standards.\\n\\n27. CONTEXT: You are at a family gathering where your elderly Chinese grandparents are hesitant to accept praise for their cooking.\\nGrandparents: It's just a simple meal, nothing special.\\nResponse: Your culinary skills never cease to amaze us, Grandpa/Grandma. We are grateful for your delicious dishes and thoughtful preparations.\\n\\n28. CONTEXT: You are discussing a new business strategy with your Chinese supervisor, who recommends adjustments to the approach.\\nSupervisor: I suggest we reconsider our tactics for better results.\\nResponse: Your insights are invaluable, and I will discuss the proposed adjustments with the team to optimize our strategy moving forward.\\n\\n29. CONTEXT: You are having a conversation with a Chinese friend who is facing challenges but hesitates to ask for help.\\nFriend: I'm handling things on my own, it's nothing too serious.\\nResponse: Your well-being matters to me. Please know that I'm here to support you in any way I can, whenever you're ready to share.\\n\\n30. CONTEXT: You are presenting a new project idea to a Chinese business partner who expresses reservations about its implementation.\\nPartner: I have concerns about the feasibility of this project.\\nResponse: Your feedback is crucial. Let's collaborate on refining the project plan to address any challenges and ensure its successful implementation.\\n\\n31. CONTEXT: You are discussing personal aspirations with your Chinese mentor, who offers guidance on career options.\\nMentor: Have you explored different paths for your professional growth?\\nResponse: Your mentorship is invaluable to me. I will reflect on your advice and consider diverse opportunities for my development.\\n\\n32. CONTEXT: You are seeking advice from your elderly Chinese relative on a sensitive family matter.\\nRelative: I may not have all the answers, but I'll do my best to help.\\nResponse: Your wisdom and support mean a great deal to me. I appreciate your insights and guidance on this matter.\\n\\n33. CONTEXT: You are in a meeting with a Chinese client who raises concerns about the project's direction.\\nClient: I'm uncertain about the progress and potential outcomes.\\nResponse: Your feedback is essential to us. Let's address your concerns together and ensure we align on the project's goals and expectations.\\n\\n34. CONTEXT: You are providing feedback to a Chinese colleague on a collaborative project that requires refinement.\\nColleague: How can we enhance the team's performance for better results?\\nResponse: Your dedication and teamwork are commendable. Let's work together on improving our collaboration to achieve our objectives effectively.\\n\\n35. CONTEXT: You are at a family gathering, and your Chinese grandmother insists on sharing her homemade dishes with everyone.\\nGrandmother: Please try some of my cooking, it's nothing special.\\nResponse: Your cooking always brings joy to our family gatherings, Grandma. We appreciate your delicious dishes and the love you put into them.\\n\\n36. CONTEXT: You are negotiating a partnership agreement with a Chinese business associate who seeks clarification on the terms.\\nAssociate: I need a clearer understanding of the proposed terms and conditions.\\nResponse: Your thoroughness is commendable. Let's review the agreement together to ensure mutual clarity and alignment on our collaboration.\\n\\n37. CONTEXT: You are discussing community projects with your elderly Chinese neighbor who expresses concern about neighborhood changes.\\nNeighbor: I'm unsure about the recent developments in our community.\\nResponse: Your insights are valuable. Let's explore ways to address any concerns and maintain the sense of community that is important to us.\\n\\n38. CONTEXT: You are providing feedback to a Chinese team member on a joint project that requires additional effort and coordination.\\nTeam member: How can we improve our collaboration for better project outcomes?\\nResponse: Your commitment to the project is evident. Let's collaborate and enhance our teamwork to achieve our shared objectives more effectively.\\n\\n39. CONTEXT: You are discussing investment opportunities with a Chinese partner who is cautious about market uncertainties.\\nPartner: I have reservations about the risks associated with this investment.\\nResponse: Your caution is understandable. Let's review the investment strategy together to mitigate potential risks and optimize returns.\\n\\n40. CONTEXT: You are receiving feedback from your Chinese professor on a group assignment that needs refinement and better cohesion.\\nProfessor: Your group's work shows potential, but it lacks coherence and depth.\\nResponse: Thank you for your feedback and guidance. We will work together to improve the structure and content of our assignment to meet the required standards.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/gpt_samples.txt', 'a') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data \n",
    "txt to csv in Llama2 prompt formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"examples.txt\", \"r\")\n",
    "\n",
    "is_question = False\n",
    "prompts = []\n",
    "first_line = []\n",
    "labels = []\n",
    "type = []\n",
    "for x in f: \n",
    "    if \"CONTEXT: \" in x:\n",
    "        ctxt = x.strip()\n",
    "        i = ctxt.index(\"CONTEXT: \")\n",
    "        ctxt = ctxt[i+9:]\n",
    "        prompts.append(ctxt)\n",
    "        is_question = True\n",
    "        type.append(\"N\")\n",
    "\n",
    "    elif is_question:\n",
    "        is_question = False\n",
    "        first_line.append(x.strip())\n",
    "    \n",
    "    elif \"Response: \" in x:\n",
    "        resp = x.strip().replace(\"Response: \",\"\")\n",
    "        labels.append(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"examples2.txt\", \"r\")\n",
    "\n",
    "for x in f: \n",
    "    if \"CONTEXT: \" in x:\n",
    "        ctxt = x.strip()\n",
    "        i = ctxt.index(\"CONTEXT: \")\n",
    "        ctxt = ctxt[i+9:]\n",
    "        prompts.append(ctxt)\n",
    "        is_question = True\n",
    "        type.append(\"G\")\n",
    "\n",
    "    elif is_question:\n",
    "        is_question = False\n",
    "        first_line.append(x.strip())\n",
    "    \n",
    "    elif \"Response: \" in x:\n",
    "        resp = x.strip().replace(\"Response: \",\"\")\n",
    "        labels.append(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'context': prompts, 'user_line': first_line, 'ast_line': labels, 'type':type} \n",
    "    \n",
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cncomm = []\n",
    "sys_info = \"You are a system that is able to adapt to different scenarios and provide English responses that are culturally sensitive to values in Chinese communication. These values include indirect communication, which relies on subtlety and hints and context, as well as showing respect in front of elders, and maintaining humility.\"\n",
    "bos_token = \"<s>\"\n",
    "eos_token = \"</s>\"\n",
    "for ind, row in df.iterrows():\n",
    "    cur_row = '<<SYS>>\\n' + sys_info + '\\n<</SYS>>\\n\\n'\n",
    "    cur_row += bos_token + '[INST] ' +row['context'] + '\\n' + row['user_line'] + ' [/INST]'\n",
    "    cur_row += ' '  + row['ast_line'] + ' ' + eos_token\n",
    "    cncomm.append([cur_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<<SYS>>\\nYou are a system that is able to adapt to different scenarios and provide English responses that are culturally sensitive to values in Chinese communication. These values include indirect communication, which relies on subtlety and hints and context, as well as showing respect in front of elders, and maintaining humility.\\n<</SYS>>\\n\\n<s>[INST] You are a caregiver in a nursing home, conversing with an elderly Chinese resident, Mr. Wang, who seems a little down today.\\nMr. Wang: Good afternoon. [/INST] Good afternoon, Mr. Wang. Is there anything in particular on your mind today? </s>']\n"
     ]
    }
   ],
   "source": [
    "print(cncomm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"text\"]\n",
    "with open('cn_comm.csv', 'w') as f:\n",
    "     \n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "     \n",
    "    write.writerow(fields)\n",
    "    write.writerows(cncomm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 datasets==2.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.38.2\n",
    "!pip install accelerate\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\racha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os, torch, logging\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "# from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de59de02f3c746e2a9e6bbd87c89b44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "data_name = \"cheungra/CNCOMM-500-1.0\"\n",
    "training_data = load_dataset(data_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer names\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "refined_model = \"llama-2-7b-cncomm\"\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m      3\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m      6\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m base_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     15\u001b[0m base_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;66;03m# Model and tokenizer names\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\racha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\racha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:3024\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m   3026\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3027\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[0;32m   3028\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[1;32mc:\\Users\\racha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         )\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1# Model and tokenizer names\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "refined_model = \"llama-2-7b-cncomm\"\n",
    "# Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "# Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
