{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sample_files(file):\n",
    "    f = open(file, \"r\")\n",
    "    is_question = False\n",
    "    prompts = []\n",
    "    ideal = []\n",
    "    for x in f: \n",
    "        x = x.strip()\n",
    "        if \"CONTEXT: \" in x:\n",
    "            i = x.index(\"CONTEXT: \")\n",
    "            cur_prompt = x[i+9:]\n",
    "            is_question = True\n",
    "\n",
    "        elif is_question:\n",
    "            cur_prompt += \"\\n\" + x\n",
    "            is_question = False\n",
    "            prompts.append(cur_prompt)\n",
    "        \n",
    "        elif \"Response: \" in x:\n",
    "            resp = x.strip().replace(\"Response: \",\"\")\n",
    "            ideal.append(resp)\n",
    "    f.close()\n",
    "    return prompts, ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama2 via Huggingface: Benchmark Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577881939a38450db2b28e63cbd0e6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(text):\n",
    "    base_prompt = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\"\n",
    "    \n",
    "    input = base_prompt.format(system_prompt = \"You are a system that is able to adapt to different scenarios and provide English responses that are culturally sensitive to values in Chinese communication. These values include indirect communication, which relies on subtlety and hints and context, as well as showing respect in front of elders, and maintaining humility.\",\n",
    "                               user_prompt = text)\n",
    "    # print(input)\n",
    "    \n",
    "    sequences = pipeline(\n",
    "        input,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=250,\n",
    "        return_full_text=False,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    output = \"\"\n",
    "    for seq in sequences:\n",
    "        output += seq['generated_text']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Benchmark Samples:\n",
    "bm_prompts, bm_ideals = parse_sample_files(\"data/benchmark_samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_data = []\n",
    "prompt_id = 1\n",
    "for sample in tqdm(zip(bm_prompts, bm_ideals)):\n",
    "    input = sample[0]\n",
    "    ideal_output = sample[1]\n",
    "    for _ in range(3):\n",
    "        cur_sample = dict()\n",
    "\n",
    "        output = get_response(input)\n",
    "        cur_sample['prompt_id'] = prompt_id\n",
    "        cur_sample['prompt'] = input\n",
    "        cur_sample['output'] = output\n",
    "        cur_sample['ideal_output'] = ideal_output\n",
    "        bm_data.append(cur_sample)\n",
    "        print(cur_sample )\n",
    "    prompt_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_df = pd.DataFrame().from_dict(bm_data) \n",
    "bm_df.to_csv(\"data/bm_10.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing llama-2-7b-cncomm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Adapter via PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "adapter_model_id = \"cheungra/llama-2-7b-cncomm\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "ft_model = PeftModel.from_pretrained(model, adapter_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = ft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = ft_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Benchmark Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ft_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(text):\n",
    "    base_prompt = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\"\n",
    "    \n",
    "    input = base_prompt.format(system_prompt = \"You are a system that is able to adapt to different scenarios and provide English responses that are culturally sensitive to values in Chinese communication. These values include indirect communication, which relies on subtlety and hints and context, as well as showing respect in front of elders, and maintaining humility.\",\n",
    "                               user_prompt = text)\n",
    "    # print(input)\n",
    "    \n",
    "    sequences = pipeline(\n",
    "        input,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=250,\n",
    "        return_full_text=False,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    output = \"\"\n",
    "    for seq in sequences:\n",
    "        output += seq['generated_text']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Benchmark Prompts:\n",
    "bm_prompts, bm_ideals = parse_sample_files(\"data/benchmark_samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cncomm_bm_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_id = 1\n",
    "for sample in tqdm(zip(bm_prompts, bm_ideals)):\n",
    "    input = sample[0]\n",
    "    ideal_output = sample[1]\n",
    "    for _ in range(3):\n",
    "        cur_sample = dict()\n",
    "\n",
    "        output = get_response(input)\n",
    "        cur_sample['prompt_id'] = prompt_id\n",
    "        cur_sample['prompt'] = input\n",
    "        cur_sample['output'] = output\n",
    "        cur_sample['ideal_output'] = ideal_output\n",
    "        cncomm_bm_data.append(cur_sample)\n",
    "        print(cur_sample)\n",
    "    prompt_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_df = pd.DataFrame().from_dict(cncomm_bm_data) \n",
    "bm_df.to_csv(\"data/cncomm_bm_10.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
